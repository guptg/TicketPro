Questions to ask:

1. Do I want to take common words (i.e. of, and, etc) out of the corpus?
2. Word tokenization vs subword tokenization, should I use something like SentencePiece for tokenization?
3. What parameters can I tweak here?
4. Will more training data help?
5. What effect does LSTM have?



Sagemaker:
Notebook instance
Pick an EC2 instace: tiny (p is for GPU), medium, version (family + size + version)
Your EC2 instance will have multiple cores, when you run your notebook everything will through all cores, multithreading - you can resize EC2 and EBS on the fly
Add EBS Volume (stores data, by default stores GB)
Add or create a git repo (automaically installed on notebook instance when its created?)
Other settings: encryption + internet access + lifecycle Config (bash script?)


Training models via Sagemaker
Elastic inference is a portion of GPU ou can attach to EC2 instance
Opening notebook instance will take you to a jupyter notebook (you will find example notebooks here, these are opensource)
You can create new notebooks and terminal which starts your EBS volume (this is valuable)
Import sagemaker libraries (look at example)
get_execution_role grabs IAM policy associated with notebook and gives access to s3 bucket


Builtin Algorithms 
Categorized according to prediction problem 
They exist in the ECR, elastic container registry, docker container, versioned
You get a container for the algorithm you want to use
Whitepapers are available 
Label in 1st coloumn - in dataset 
Be aware of hyperparameters 
With built in algorithms you are essentially passing a json file describing the training environment and your container with the algorithm
Something it can do for example is train 32 different versions of the model automatically and give you the best one 

Script mode
AWS sagemaker manages a container that lives in ECR already
Choose tf container, pytorch container, etc, 
Put the code inside the containers 
Specify the entry point for the sagemaker estimator (file path)
include any extra libraries with a requirements.txt 
Use web server for inference 

Bring in your own docker file 
Point the model within docer file 
Register that container on ECR
point to container
include serve() function

AWS ML Marketplace - premade solution 

Training
EC2 instances alive for the number of seconds your model is training while you develop on another EC2 instance
An estimator specifies everything, container, role associated with that notebook, EC2instance count, EC2 type, EBS volume size
Pick hyperparameter ranges 
Pick Performance evaluation technique via the objective_metric_name, i.e. AUC from validation 
Choose number of jobs (number of models you want to train), optimizes hyperparameters at each step in time using a baysian optimzer (you can also use random search)

Data
sent through channels via s3
y column is the target in the dataset
y must be first in the dataset 

model.fit() --> spins up a cluster 

When bringing your own algorithm, set hyperparameters as a JSON object

Bringing your own script:
0. Choose container
1. specify estimator, specify the python script (pass arguments to be compatible with sagemaker and save model as Tensforflow service)
2. requirements.txt or pip3 install in notebook instance?
2. estimator.fit()
4. Is data in s3? Copy it to EBS volume of EC2 instance in s3, separate data and validation datsets, copy training script for convenience as well 
